# 8.6 Пример: параллельный веб-сканер

В разделе 5.6 мы создали простой веб-сканер, который исследовал граф ссылок в Интернете в порядке поиска в ширину. В
этом разделе мы сделаем его параллельным, чтобы независимые вызовы `crawl` могли использовать параллелизм операций
ввода-вывода в Интернете. Функция `crawl` остается той же, что и в ch5/findlinks3:

``` go
func crawl(url string) []string {
	fmt.Println(url)
	list, err := links.Extract(url)
	if err != nil {
		log.Print(err)
	}
	return list
}
```

Функция `main` напоминает `breadthFirst` (раздел 5.6). Как и ранее, `worklist` записывает очередь элементов, требующих
обработки, каждый элемент списка URL сканируется, но на этот раз вместо представления очереди в виде среза мы используем
`канал`. Каждый вызов `crawl` осуществляется в своей горутине и передает обнаруженные ссылки обратно в список:

``` go
func main() {
	worklist := make(chan []string)
	
	// Запуск с аргументами командной строки.
	go func() { worklist <- os.Args[1:] }()
	
	// Параллельное сканирование.
	seen := make(map[string]bool)
	for list := range worklist {
		for _, link := range list {
			if !seen[link] {
				seen[link] = true
				go func(link string) {
					worklist <- crawl(link)
				}(link)
			}
		}
	}
}
```

Обратите внимание, что горутина сканирования принимает `link` в качестве явного параметра, чтобы избежать проблемы
`захвата переменной цикла`, которую мы впервые увидели в разделе 5.6.1. Обратите также внимание, что первоначальные
аргументы командной строки должны передаваться в рабочий список в отдельной горутиной, чтобы избежать
`взаимоблокировки`, ситуации "зависания", в которой основная горутина и горутины сканирования пытаются
отправлять информацию друг другу в то время, когда ни одна из них ее не получает. Альтернативное решение заключается в
использовании буферизованного канала.

Теперь сканер обладает высокой параллельностью и выводит поток URL, но в нем имеются две проблемы. Первая проблема
проявляется в виде сообщений об ошибках в журнале после нескольких секунд работы:

``` shell
$ go build crawl1.go 
$ ./crawl1 http://gopl.io/ 
http://gopl.io/ 
https://golang.org/help/ 
https://golang.org/doc/ 
https://golang.org/blog/
...
2015/07/15 18:22:12 Get ...: dial tcp: lookup blog.golang.org: нет такого хоста
2015/07/15 18:22:12 Get ...: dial tcp 23.21.222.120:443: socket: слишком много открытых файлов
...
```

Первое сообщение — это странное уведомление об ошибке поиска DNS для существующего домена. Следующее сообщение об ошибке
поясняет причины: программа создает так много сетевых подключений сразу, что превышает предел количества одновременно
открытых файлов, установленный для каждого процесса, что приводит к сбоям таких операций, как DNS-запросы и вызовы
`net.Dial`.

Эта программа оказывается `слишком параллельной`. Неограниченный параллелизм редко является хорошей идеей, поскольку в
системе всегда есть ограничивающие факторы, такие как количество ядер процессора для вычислительной нагрузки, количество
шпинделей и головок для локальных дисковых операций ввода-вывода, пропускной способности сети для потоковой загрузки.
Решение заключается в том, чтобы ограничить количество параллельных обращений к ресурсам в соответствии с доступным
уровнем параллелизма. Простейший способ добиться этого в нашем примере — обеспечить не более чем `n`
вызовов `links.Extract` одновременно, где n — значение, меньшее, чем ограничение на количество открытых файлов (скажем,
20). Это аналогично тому, как швейцар в переполненном ночном клубе впускает гостя только тогда, когда уйдут некоторые
другие гости.

Мы можем ограничить параллелизм с помощью буферизованного канала с емкостью `n` для моделирования примитива
параллелизма,
который называется `подсчитывающий семафором`. Концептуально каждый из `n` свободных слотов в буфере канала представляет
маркер. Отправление значения в канал захватывает маркер, а получение значения из канала его освобождает, создавая новый
свободный слот. Это гарантирует, что без промежуточного получения значения из канала в него может быть отправлено не
более `n` значений. (Хотя более наглядным может быть рассмотрение как маркеров заполненных слотов в буфере канала, но
использование свободных слотов позволяет избежать необходимости заполнять буфер канала после его создания). Так как тип
элемента канала не важен, мы будем использовать `struct{}`, имеющий нулевой размер.

Давайте перепишем функцию `crawl` так, чтобы вызов `links.Extract` был помещен между операциями захвата и освобождения
маркера, тем самым гарантируя, что одновременно активными являются не более `20` вызовов. Хорошая практика заключается в
том, чтобы **операции с семафорами располагались как можно ближе к операциям ввода-вывода**, которые они регулируют.

``` go
// tokens представляет собой подсчитывающий семафор, используемый
// для ограничения количества параллельных запросов величиной 20.
var tokens = make(chan struct{}, 20)

func crawl(url string) []string {
	fmt.Println(url)
	tokens <- struct{}{} // Захват токена
	list, err := links.Extract(url)
	<-tokens // Освобождение токена
	if err != nil {
		log.Print(err)
	}
	return list
}
```

Вторая проблема заключается в том, что программа никогда не завершается, даже после обнаружения всех достижимых из
начального URL ссылок. (Конечно, вы вряд ли заметите эту проблему, если только не выберете тщательно первоначальный URL
или реализуете возможность ограничения глубины из упражнения 8.6.) Для завершения программы мы должны выйти из основного
цикла, когда список пуст и нет активных сканирующих горутин.

``` go
func main() {
	worklist := make(chan []string)
	var n int // Количество ожидающих отправки в рабочий список

	// Запуск с аргументами командно строки.
	n++
	go func() { worklist <- os.Args[1:] }()

	// Параллельное сканирование веб.
	seen := make(map[string]bool)
	for ; n > 0; n-- {
		list := <-worklist
		for _, link := range list {
			if !seen[link] {
				seen[link] = true
				n++
				go func(link string) {
					worklist <- crawl(link)
				}(link)
			}
		}
	}
}
```

В этой версии счетчик `n` отслеживает количество отправок в рабочий список, которые находятся в состоянии ожидания.
Каждый раз, зная, что элемент должен быть отправлен в рабочий список, мы увеличиваем `n`, один раз перед тем, как
отправлять в список первоначальные аргументы командной строки, и по разу перед каждым запуском горутины сканирования.
Главный цикл завершается, когда `n` уменьшается до нуля, поскольку при этом больше нет никакой работы, которую предстоит
сделать. Теперь параллельный сканер работает (без ошибок) примерно в 20 раз быстрее сканера из раздела 5.6 и корректно
завершается по окончании работы.

В приведенной ниже программе показано альтернативное решение проблемы чрезмерного параллелизма. Эта версия использует
исходную функцию `crawl` без подсчитывающего семафора, но вызывает ее из одной из 20 долгоживущих горутин
сканирования, гарантируя тем самым, что одновременно активны не более 20 НТТР-запросов.

``` go
func main() {
	worklist := make(chan []string)  //  Список URL, могут быть дубли.
	unseenLinks := make(chan string) // Удаление дублей.

	// Добавление в список аргументов командной строки
	go func() { worklist <- os.Args[1:] }()

	// Создание 20 горутин сканирования для выборки всех непросмотренных ссылок.
	for i := 0; i < 20; i++ {
		go func() {
			for link := range unseenLinks {
				foundLinks := crawl(link)
				go func() { worklist <- foundLinks }()

			}
		}()
	}

	// Главная горутина удаляет дубликаты из списка и отправляет непросмотренные ссылки сканерам.
	seen := make(map[string]bool)
	for list := range worklist {
		for _, link := range list {
			if !seen[link] {
				seen[link] = true
				unseenLinks <- link
			}
		}
	}
}
```

Горутины сканеров используют один и тот же канал, `unseenLinks`. Главная горутина отвечает за исключение
дублирования элементов, которые она получает из рабочего списка, а затем отправляет каждую непросмотренную ссылку по
каналу `unseenLinks` горутине сканирования.

Карта `seen` `замкнута` в пределах главной горутины, т.е. доступ к нему имеет только главная горутина.
Подобно другим разновидностям сокрытия информации, `замкнутость` помогает нам обосновать корректность программы.
Например, локальные переменные не могут упоминаться по имени за пределами функции, в которой они были объявлены;
переменные, не сбежавшие (раздел 2.3.4) из функции, недоступны вне этой функции; а инкапсулированные поля объекта не
могут быть доступны ничему, кроме методов этого объекта. Во всех случаях сокрытие информации помогает ограничить
нежелательные взаимодействия между частями программы.

Ссылки, найденные функцией `crawl`, отправляются в рабочий список из выделенной горутины, чтобы избежать
взаимоблокировки.

Для экономии места мы не рассматривали в этом примере проблему завершения программы.

## Выводы:

* Для избежания проблемы захвата переменной цикла, горутина сканирования принимает параметр ссылки, а не использует
  переменную цикла напрямую;
* Передача аргументов командной строки в рабочий список осуществляется в отдельной горутине, чтобы избежать
  взаимоблокировки между основной горутиной и горутинами сканирования;
* Слишком высокая параллельность может привести к проблемам, таким как превышение ограничения на количество одновременно
  открытых файлов и сбоев сетевых операций;
* Ограничение количества параллельных обращений к ресурсам может быть достигнуто путем контроля числа одновременно
  выполняемых функций с использованием буферизованного канала и подсчитывающего семафора;
* Каждый свободный слот в буфере канала представляет маркер, который занимается при выполнении функции и освобождается
  при завершении, что обеспечивает ограничение параллелизма;
* Использование подсчитывающего семафора для ограничения параллельных запросов: ограничивает количество одновременно
  выполняющихся HTTP-запросов, что позволяет достичь баланса между производительностью и нагрузкой на сервер;
* Включение счетчика для отслеживания количества отправок в рабочий список (var n int): позволяет определить, работает
  ли программа и следует ли ожидать завершения работы. Это используется для корректного завершения программы после
  обнаружения всех достижимых ссылок из начального URL;
* Создание основного цикла для удаления дубликатов ссылок и отправки непросмотренных ссылок сканерам: уменьшает
  количество одинаковых запросов и делает код более эффективным;
* Использование замкнутой переменной в главной горутине для обеспечения сокрытия информации: делает программу более
  безопасной и корректной, защищая переменные от несанкционированного доступа другими частями программы.